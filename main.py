import argparse
import logging
import os
from logger_config import setup_logging
from data_load import configure_gpu, verify_dataset
from data_spliting import run_splitting
from image_process import prepare_datasets
from data_augmentation import apply_augmentation
# ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ù„Ø© ÙˆØ§Ø³Ù… Ø§Ù„Ù…Ù„Ù
from model_pipeline import run_full_mlops_lifecycle 

setup_logging()
logger = logging.getLogger("MainPipeline")

def main():
    try:
        # --- Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª ---
        parser = argparse.ArgumentParser()
        parser.add_argument("--lr_stage1", type=float, default=0.0001)
        parser.add_argument("--lr_stage2", type=float, default=0.00005)
        parser.add_argument("--epochs_stage1", type=int, default=10)
        parser.add_argument("--epochs_stage2", type=int, default=40)
        args = parser.parse_args()

        logger.info("ğŸ¬ Starting the Full Plant Disease MLOps Pipeline...")

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆÙØ­Øµ Ø§Ù„Ø¯Ø§ØªØ§ ---
        configure_gpu()
        RAW_DATA_PATH = "/home/youssef/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3/plantvillage dataset/color"
        
        if not os.path.exists(RAW_DATA_PATH):
             raise FileNotFoundError(f"Source dataset not found at {RAW_DATA_PATH}")

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¯Ø§ØªØ§ ---
        logger.info("ğŸ“‚ Splitting data...")
        run_splitting() 
        
        SPLIT_BASE = '/home/youssef/plant_disease_split'
        TRAIN_DIR = os.path.join(SPLIT_BASE, 'train')
        VAL_DIR = os.path.join(SPLIT_BASE, 'val')
        TEST_DIR = os.path.join(SPLIT_BASE, 'test')

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---
        logger.info("âš™ï¸ Preprocessing images...")
        # ØªØ£ÙƒØ¯ Ø¥Ù† prepare_datasets Ø¨ØªØ±Ø¬Ø¹ Ø§Ù„Ù€ 4 Ù‚ÙŠÙ… Ø¯ÙˆÙ„ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
        train_ds, val_ds, test_ds, class_names = prepare_datasets(
            TRAIN_DIR, VAL_DIR, TEST_DIR, 
            img_size=(224, 224), 
            batch_size=16
        )
        
        train_ds = apply_augmentation(train_ds)

        # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: MLOps Lifecycle ---
        logger.info("ğŸš€ Launching MLflow Lifecycle...")
        
        # Ù†Ù…Ø±Ø± Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø¯Ø§Ù„Ø© ÙÙŠ model_pipeline
        run_id = run_full_mlops_lifecycle(
            train_ds=train_ds, 
            val_ds=val_ds, 
            test_ds=test_ds,
            lr_stage1=args.lr_stage1,
            lr_stage2=args.lr_stage2,
            epochs_stage1=args.epochs_stage1,
            epochs_stage2=args.epochs_stage2
        )

        logger.info(f"âœ… Pipeline Finished! Run ID: {run_id}")
        logger.info("ğŸ”— Run 'mlflow ui' to see results.")

    except Exception as e:
        logger.critical(f"ğŸ’¥ Pipeline failed: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()